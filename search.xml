<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>FaceBoxes: 高精度的CPU实时人脸检测器</title>
      <link href="2020/11/22/faceboxes-gao-jing-du-de-cpu-shi-shi-ren-lian-jian-ce-qi/"/>
      <url>2020/11/22/faceboxes-gao-jing-du-de-cpu-shi-shi-ren-lian-jian-ce-qi/</url>
      
        <content type="html"><![CDATA[<h1 id="FaceBoxes-高精度的CPU实时人脸检测器"><a href="#FaceBoxes-高精度的CPU实时人脸检测器" class="headerlink" title="FaceBoxes: 高精度的CPU实时人脸检测器"></a>FaceBoxes: 高精度的CPU实时人脸检测器</h1><p>论文题目：《FaceBoxes: A CPU Real-time Face Detector with High Accuracy》</p><p>论文链接：<a href="https://arxiv.org/pdf/1708.05234.pdf">https://arxiv.org/pdf/1708.05234.pdf</a></p><p>年份：2017</p><p>论文作者：Shifeng Zhang等人</p><p>作者单位：中国科学院自动化研究所等</p><p>公众号CVpython同步发布</p><h2 id="1-论文要解决什么问题？"><a href="#1-论文要解决什么问题？" class="headerlink" title="1. 论文要解决什么问题？"></a>1. 论文要解决什么问题？</h2><p>要保持高精度，还要在CPU上达到实时？还真有点难，但是Shifeng Zhang等人针对这个问题，提出了人脸检测模型FaceBoxes，表现SOTA。</p><h2 id="2-FaceBoxes如何解决问题？"><a href="#2-FaceBoxes如何解决问题？" class="headerlink" title="2. FaceBoxes如何解决问题？"></a>2. FaceBoxes如何解决问题？</h2><p>FaceBoxes框架如图1所示，主要包括Rapidly Digested Convolutional Layers (RDCL)和Multiple Scale Convolutional Layers (MSCL)模块，还有anchor密集策略。</p><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/faceboxes.PNG" alt="img"></p><h3 id="2-1-RDCL"><a href="#2-1-RDCL" class="headerlink" title="2.1 RDCL"></a>2.1 RDCL</h3><p>RDCL的目的是为了快速下采样，让模型能够在CPU上面能达到实时。RDCL采用的方法是缩小空间大小，选择合适的卷积核大小和减少输出通道。</p><ul><li>缩小空间大小：Conv1, Pool1, Conv2 and Pool2 的步长分别是4, 2, 2和 2, 空间大小快速降低了32倍。</li><li>合适的卷积核大小：前几层的一些核应该是比较小，以便加速，但是也应该足够大，以减轻空间大小减小而带有的信息丢失(为什么可以减少信息丢失s)。Conv1, Conv2的核大小为7x7, 5x5，所有池化层的核大小为3x3。</li><li>减少输出通道：使用C.ReLU减少输出通道，操作如图2(a)所示。因为C.ReLU作者统计发现底层卷积时卷积核存在负相关，也就是说假设我们本来使用10个卷积核，但是现在只需要用5个卷积核，另外5个卷积核的结果可以通过负相关得到。结果表明使用C.ReLU加速的同时也没损失精度。</li></ul><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/faceboxes1.PNG" alt="img"></p><h3 id="2-2-MSCL"><a href="#2-2-MSCL" class="headerlink" title="2.2 MSCL"></a>2.2 MSCL</h3><p>MSCL是为了得到更好地检测不同尺度的人脸。</p><ul><li>深度：在MSCL模块中，随着网络的加深，便得到不同大小的特征映射(多尺度特征)。在不同大小特征映射中设置不同大小的anchor，有利于检测不同大小的人脸。</li><li>宽度：Inception由多个不同核大小的卷积分支组成。在这些分支中，不同的网络宽度，也有不同大小的特征映射。通过Inception，感受野也丰富了一波，有利用检测不同大小的人脸。</li></ul><p>MSCL在多个上尺度进行回归和分类，在不同尺度下检测不同大小的人脸，能够大大提高检测的召回率。</p><h3 id="2-3-Anchor密度策略"><a href="#2-3-Anchor密度策略" class="headerlink" title="2.3 Anchor密度策略"></a>2.3 Anchor密度策略</h3><p>Inception3的anchor大小为32，64和128，而Conv3_2和Con4_2的anchor大小分别为256，512。anchor的平铺间隔等于anchor对应层的步长大小。例如，Con3_2的步长是64个像素点，anchor大小为256x256，这表明在输入图片上，每隔64个像素就会有一个256x256的anchor。关于anchor的平铺密度文中是这样定义的：</p><script type="math/tex; mode=display">A_{density}=A_{scale}/A_{interval}</script><p>其中$A_{density}$和$A_{interval}$分别为anchor的尺度和平铺间隔。默认的平铺间隔(等于步长)默分别认为32，32，32，64和128。所以Inception的平铺密度分别为1,2,4，而Con3_2和Con4_2的平铺密度分别为4,4。</p><p>可以看出来，不同尺度的anchor之间存在平铺密度不平衡的问题，导致小尺度的人脸召回率比较低，因此，为了改善小anchor的平铺密度，作者提出了anchor密度策略。为了使anchor密集n倍，作者均匀地将$A_{number}=n^2$个anchor铺在感受野的中心附近，而不是铺在中心，如图3所示。将32x32的anchor密集4倍，64x64的anchor密集两倍，以保证不同尺度的anchor有相同的密度。</p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201121200018610.png" alt="image-20201121200018610"></p><h3 id="2-4-训练"><a href="#2-4-训练" class="headerlink" title="2.4 训练"></a>2.4 训练</h3><p><strong>数据扩增</strong>：</p><ul><li>颜色扭曲</li><li>随机采样</li><li>尺度变换</li><li>水平翻转</li><li>Face Boxes过滤：经过数据扩增的图片中，如果face boxes的中心还在图片上，则保留重叠部分，然后把高或者宽&lt;20的过滤掉(这个操作不是很懂了，为什么把小目标过滤掉？)</li></ul><p><strong>匹配策略</strong>：训练期间，需要确定哪些anchor对应脸部的bounding box，我们首先用最佳jaccard重叠将每一张脸匹配到anchor，然后将anchor匹配到jaccard重叠大于阈值的任何一张脸。</p><p><strong>Loss function</strong>: 对于分类，采用softmax loss，而回归则采用smooth L1 损失。</p><p><strong>Hard negative mining</strong>：anchor 匹配后，发现很多anchor是负的，这会引入严重的正负样本不平衡。为了快速优化和稳定训练，作者对loss进行排序然后选择最小的，这样子使得负样本和正样本的比例最大3:1。</p><h2 id="3-实验结果如何？"><a href="#3-实验结果如何？" class="headerlink" title="3 实验结果如何？"></a>3 实验结果如何？</h2><p><strong>Runtime</strong></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20201121212045119.png" alt="image-20201121212045119"></p><p><strong>Evaluation on benchmark</strong></p><p>在FDDB上SOTA。</p><h2 id="4-对我们有什么指导意义？"><a href="#4-对我们有什么指导意义？" class="headerlink" title="4.对我们有什么指导意义？"></a>4.对我们有什么指导意义？</h2><ul><li>要在CPU上达到实时，考虑一开始就对特征进行快速下采样。</li><li>在浅的卷积层考虑使用CReLU，可以减少计算量。</li><li>MSCL告诉我们，检测各种不同大小的物体，考虑从深度和宽度上丰富感受野。</li><li>anchor密度策略告诉我们考虑anchor密度以提高召回率。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 目标检测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络可视化:Grad-CAM</title>
      <link href="2020/11/15/shen-jing-wang-luo-ke-shi-hua-grad-cam/"/>
      <url>2020/11/15/shen-jing-wang-luo-ke-shi-hua-grad-cam/</url>
      
        <content type="html"><![CDATA[<p>论文题目：《Grad-CAM:Visual Explanations from Deep Networks via Gradient-based Localization》</p><p>作者单位：Georgia Institute of Technology，Facebook AI Research</p><p>年份：2017</p><p>公众号: CVpython 同步发布</p><p>导语：前段时间，用Grad-CAM来对神经网络的输出进行可视化，当时做的是一个多便签分类任务，但是可视化出来的结果感觉都点怪怪的，总感觉哪里不对。这次论文的总结让我对Gradm-CAM有了进一步的理解，终于知道当时可视化问题究竟在哪了，Oh yeah!。</p><h2 id="1-论文要解决什么问题？"><a href="#1-论文要解决什么问题？" class="headerlink" title="1.论文要解决什么问题？"></a>1.论文要解决什么问题？</h2><p>虽然CNN模型在CV领域取得了很大的突破，但是CNN就像一个“黑盒子一样”，里面究竟是怎么一回事，还是很难让人明白，可解释性很差。如果模型不work，其实也很难解析清楚究竟是为什么。所以作者提出Grad-CAM模型，对CNN所做的决策作出可视化解释。</p><h2 id="2-论文所提模型如何解决问题？"><a href="#2-论文所提模型如何解决问题？" class="headerlink" title="2. 论文所提模型如何解决问题？"></a>2. 论文所提模型如何解决问题？</h2><p>很多研究都表明CNN更深的层能够捕获更高级的视觉结构信息，而且，卷积特征中空间信息会在全连接层丢失，所以在最后卷积层，我们在高级的语义信息和详细的空间信息之间能够得到最好的折衷(为什么说这是一个折衷？)。Grad-CAM利用“流进”CNN的最后一层的梯度信息来理解每个神经元对决策的重要程度。</p><p>Grad-CAM总体结构如下图所示：</p><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/Grad_CAM1.JPG" alt=""></p><p>输入图像经过前向计算得到了特征映射$A$，对于类别$c$，在softmax之前有类别分数$y^c$。现在假设$A^k_{(x,y)}$为特征映射$A$在$(x,y)$位置的第$k$个通道的值，然后计算：</p><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/Grad_CAM2.JPG" alt=""></p><p>我们先来理解一下$y^c$对$A^k_{(x,y)}$的求导能够得到什么。</p><p>先举个简单的例子，对于公式$y=w1<em>x1+w2</em>x2$，其中$x1,x2$为自变量，$w1,w2$分别是两个自变量的系数，$y$对$x1$求偏导结果为$w1$，如果$x1$对$y$更加重要，$w1$系数自然也更大，所以$y$对$x1$的偏导结果也就更大了，那是不是说明了求导可以反映出自变量对函数的重要程度？答案是显然的(如果有大佬觉得不严谨，请指点)。</p><p>所以$y^c$对$A^k_{(x,y)}$的求导能够得到什么？得到的就是$A^k_{(x,y)}$这个特征值对$y^c$的重要程度，再来一个全局平均池化，得到就是特征映射第$k$个通道对$y^c$的重要程度了。</p><p>上面公式(1)求出了特征映射每个通道的系数，然后线性组合一下，如公式(2)所示。</p><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/Grad_CAM3.JPG" alt=""></p><p>加上$ReLU$的原因是因为作者只对那些对类别分数有正向影响的特征感兴趣，所以过滤了一下那些有负面影响的特征。</p><p>虽然Grad-CAM的可视化是具有类别判别力的，并且能够定位到相关的区域，但是它缺少显示细粒度重要性的能力。例如图1(c)，虽然Grad-CAM可以定位到猫的区域，但是为什么网络将它预测为“tiger cat”，从低分辨率的heat-map很难得到结论。为了能够定位的同时也显示细粒度，作者通过点乘将Guided Backpropagation 和Grad-CAM相结合，得到Guided Grad-CAM。如图1(d,j)所示。</p><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/Grad_CAM4.JPG" alt=""></p><h2 id="3-实验结果如何？"><a href="#3-实验结果如何？" class="headerlink" title="3. 实验结果如何？"></a>3. 实验结果如何？</h2><p>定位能力更好，分类也不弱。</p><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/Grad_CAM5.JPG" alt=""></p><h2 id="4-对我们有什么指导意义？"><a href="#4-对我们有什么指导意义？" class="headerlink" title="4.对我们有什么指导意义？"></a>4.对我们有什么指导意义？</h2><p>感觉这才是最重要的点。</p><ul><li>Grad-CAM的可视化结果(包括区域和细粒度)为我们提供一个模型不work的解释，譬如某个图像分类错了，我们可视化一下，是感兴趣的区域有问题还是提取的细粒度特征有问题？</li><li>还可以验证数据集偏差，论文中有一个例子，例如识别医生和护士，可视化的结果显示模型定位的区域在人脸和发型，模型把有一些女医生识别成护士，而男护士识别成医生，存在性别偏见，以为男的就是医生，女的就是护士，看看数据集，会发现可能是因为78%的医生里面都是男的，而93%的护士都是女的，就存在数据集偏差。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 可视化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于注意力的多行人属性识别的深度学习模型</title>
      <link href="2020/11/14/ji-yu-zhu-yi-li-de-duo-xing-ren-shu-xing-shi-bie-de-shen-du-xue-xi-mo-xing/"/>
      <url>2020/11/14/ji-yu-zhu-yi-li-de-duo-xing-ren-shu-xing-shi-bie-de-shen-du-xue-xi-mo-xing/</url>
      
        <content type="html"><![CDATA[<h1 id="基于注意力的多行人属性识别的深度学习模型"><a href="#基于注意力的多行人属性识别的深度学习模型" class="headerlink" title="基于注意力的多行人属性识别的深度学习模型"></a>基于注意力的多行人属性识别的深度学习模型</h1><p>论文题目《An Attention-Based Deep Learning Model for Multiple Pedestrian Attributes Recognition 》</p><p>链接：<a href="https://arxiv.org/abs/2004.01110">https://arxiv.org/abs/2004.01110</a> </p><p>作者单位：清华大学</p><p>年份：2020</p><h2 id="1-论文主要解决什么问题？"><a href="#1-论文主要解决什么问题？" class="headerlink" title="1. 论文主要解决什么问题？"></a>1. 论文主要解决什么问题？</h2><p>行人属性预测是一个多任务学习问题。为了共享特征表达，传统的多任务学习方法通常学习特征或者特征子空间的线性组合。但是这种组合排除了通道之间的复杂的相互依赖性。更何况，空间信息交换也很少被考虑。论文提出了协同注意力共享(CAS)模型来提取具有判断力的通道和空间区域，以便在多任务学习中很好地共享特征。</p><p>说人话：以前多任务的方法真弱鸡，很多都只是把特征简单相加，不考虑特征通道信息依赖性和空间信息的交互？</p><h2 id="2-论文如何解决问题？"><a href="#2-论文如何解决问题？" class="headerlink" title="2. 论文如何解决问题？"></a>2. 论文如何解决问题？</h2><p>行人属性分类方法中，常用的网络结构如如图1所示：</p><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/CAS.PNG" alt=""></p><ul><li>Hard-Sharing 结构，但是可能容易产生负转移问题，也就是说对一个某个行人属性进行预测的时候可能容易被其他属性所影响。</li><li>Vanilla 结构，它集成了两个独立的网络结构，分别负责预测不同的属性。联系紧密的属性就分成同一个组，由同一个网络负责。但是两个网络之间没有任何的交互，一些有用的相关信息可能没有被利用起来。</li><li>Soft-Sharing 结构，集成Hard-Sharing 和Vanilla 结构的优点，每一层利用一个模块来决定哪些特征该共享哪些不该共享。</li></ul><p>之前的多任务学习的方法，譬如Cross Stich模块和Sluice模块，不同任务之间的特征交互只是通过简单的元素相加操作，忽略了通道信息。而且行人属性通常跟不同的空间位置有较大的关系。因此作者提出了协同注意力共享(CAS)模型来提取具有判断力的通道和空间位置，以便在网络间共享特征。</p><p>作者提出的CAS模型如图2所示：</p><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/2.JPG" alt=""></p><p>这一种Soft-Sharing 结构，由两个网络及其中间的交互模块组成。上下两个网络结构是一致的，输入特征$ feat $ 经过GAP(全局平均池化)得到$V_g$，然后把$V_g$“喂入”全连接层便可得到中间向量$V_m$。</p><ul><li>协同分支(Synergetic Branch)：该分支的输入为$V_{sh}$，它由上面的网络$A$和下面的网络$B$的中间向量$V_m$经过全连接层得到的结果。$V_{sh} $与该层的$feat$进行$element-wise$相乘操作，结果分别记为$feat^A$和$feat^{B}$。然后对$feat^A_{sh}$和$feat^{B}_{sh}$进行通道接拼，得到$feat_{cat} $。然后$concat(Avg(feat_{cat}),Max(feat_{cat}))$，对其结果进行卷积操作，结果记为$M$。其中$Avg$和$Max$分别是通道上的平均值和最大值函数。$feat_{cat}$经过卷积得到$feat_{sym}$。协同分支的输出便是$M$和$feat_{sym}$了。其中$M$将会被送入到注意力分支。</li><li>注意力分支(Attentive Branch)：该分支的输入为$V_a$，它由$V_m$经过全连接层所得到。然后$V_a$与协同分支的输出$M$进行$element-wise$相乘，其结果记为$A$。</li><li>任务分支(Task-specific Branch)：该分支的输入为$V_t$，它也是由$V_m$经过全连接层所得到。然后$V_a$与该层的$feat$进行$element-wise$相乘，其结果记为$feat_t$。</li><li>分支聚合：$feat$，$feat_{sym}$和$feat_t$进行$element-eise$的相加,其结果与$A$进行$element-wise$相乘。得到的结果将”喂入”下一层网络。</li></ul><h2 id="3-实验结果如何？"><a href="#3-实验结果如何？" class="headerlink" title="3. 实验结果如何？"></a>3. 实验结果如何？</h2><ul><li>结果超过了传统共享单元的方法，与SOTA的相比，也达到了更好的结果。</li></ul><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/CAS2.JPG" alt=""></p><h2 id="4-对我们有什么指导意义？"><a href="#4-对我们有什么指导意义？" class="headerlink" title="4.对我们有什么指导意义？"></a>4.对我们有什么指导意义？</h2><ul><li><p>多任务学习中，Soft-Sharing 结构优于Hard-Sharing 结构和Vanilla 结构。</p></li><li><p>空间信息对于行人属性识别还是很重要的，对特征$element-wize$相加操作可能不太有利用提取空间区域信息，但是concat操作应该还是有用的。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 行人属性 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详解可迭代对象和迭代器</title>
      <link href="2020/11/08/xiang-jie-ke-die-dai-dui-xiang-he-die-dai-qi/"/>
      <url>2020/11/08/xiang-jie-ke-die-dai-dui-xiang-he-die-dai-qi/</url>
      
        <content type="html"><![CDATA[<h1 id="可迭代对象和迭代器"><a href="#可迭代对象和迭代器" class="headerlink" title="可迭代对象和迭代器"></a>可迭代对象和迭代器</h1><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>可迭代对象和迭代器是经常碰到但又很容易混淆的两个概念，所以今天小编跟大家深入剖析一下可迭代对象和迭代器的区别。认真看完本文，你将收获：</p><ul><li>理解什么是可迭代对象</li><li>理解检查可迭代对象的方法　</li><li>理解什么是迭代器</li><li>可迭代对象和迭代器的关系</li></ul><p>事不宜迟，我们马上开始吧！</p><h2 id="可迭代对象"><a href="#可迭代对象" class="headerlink" title="可迭代对象"></a>可迭代对象</h2><p>要理解可迭代对象，那首先要搞清楚迭代的概念。关于迭代，维基百科是这样子定义的：</p><blockquote><p><strong>迭代</strong>是重复反馈过程的活动，其目的通常是为了接近并到达所需的目标或结果。每一次对过程的重复被称为一次“迭代”，而每一次迭代得到的结果会被用来作为下一次迭代的初始值。</p></blockquote><p>从这个定义中，我们大概可以知道迭代是对某一个过程的重复。其实在程序中，迭代也是类似的，它是一种遍历集合元素的方式，请看下面的示例1。</p><pre><code class="python"># 示例1for i in [1,2,3]:    print(i)</code></pre><p>输出结果：</p><pre><code class="python">123</code></pre><p>在示例1中，解释器重复地从列表中取出元素并打印，直到遍历结束为止，这就是一个迭代的过程。可见，可迭代对象可以在for循环中遍历元素。</p><p>那什么样的对象才是一个可迭代对象？<strong>事实上，只要实现 __ iter __ 方法或者实现 __ getitem __方法而且其参数从0开始索引，那么该对象就是可迭代对象</strong>，请看示例2。</p><pre><code class="python">#示例2class Vector(object):    def __init__(self,components):        self.components = list(components)    def __iter__(self):        return iter(self.components)V1 = Vector([1,2,3])for i in V1:    print(i)</code></pre><p>输出结果：</p><pre><code class="python">123</code></pre><p>从示例2中可见，Vector类实现了 __ iter __方法，解释器可以从Vector类对象中重复地取出元素并打印。如果要检查某一个对象是否为可迭代对象，其实可以使用isinstance( )函数，该函数用于判断对象是否为某一类型，但是用这个函数判断不一定准确(原因后面会说到)。</p><pre><code class="python">from collections.abc import Iterableprint(isinstance(V1,Iterable))#True</code></pre><p>如果我们只是实现了 __ getitem __ ()方法，情况又会怎么样呢？请看示例3。</p><pre><code class="python"># 示例3from collections.abc import Iterableclass Vector(object):    def __init__(self,components):        self.components = list(components)    def __getitem__(self,index):        return self.components[index]V1 = Vector([1,2,3])for i in V1:    print(i)print(isinstance(V1,Iterable))</code></pre><p>输出结果：</p><pre><code class="python">123Flase</code></pre><p>从示例3中可看到，Vector实现了 __ getitem __ 方法，解释器可以对V1进行迭代并打印元素，但是！使用isinstance()判断时，返回的结果居然是False。明明可以使用for循环来迭代元素，为什么是判断是Flase呢？事实上，如果可迭代对象只是实现了 __ getitem __ 的话，abc.Iterable是不考虑该方法的，这便导致了isinstance()判断不准确。<strong>更准确的方法应该是调用iter()函数</strong>，如果该对象不可迭代，就会抛出TypeError的错误。我们尝试使用iter()来判断一下。</p><pre><code class="python"># 示例4print(iter(V1))#&lt;iterator object at 0x000001B262E35518&gt;#去掉__getitem__方法后print(iter(V1))#TypeError: &#39;Vector&#39; object is not iterable</code></pre><p>从示例4中可以看到，对于可迭代对象，iter()会返回&lt; iterator object at xxxx &gt;。当去掉 __ getitem __ ()方法后再检查时，便抛出TypeError错误。iter()函数用于生成一个迭代器，也就是说<strong>可以返回一个迭代器的就是一个可迭代对象。</strong></p><p>该对象之所以能迭代，是因为实现了__ iter __ ()方法。当使用for循环时候，解释器会检查对象是否有__ iter __ ()方法，有的话就是调用它来获取一个迭代器。所以没有 __ iter __ ()方法但实现了__ getitem __ ()，解释器会创建一个迭代器，尝试从0开始按顺序遍历元素。如果尝试失败，Python便会抛出TypeError错误。</p><p>那么Python内置类型中究竟有哪些可迭代对象呢？我们一起盘点一下吧。</p><ul><li>list</li><li>dict</li><li>tuple</li><li>set</li><li>string</li></ul><p>其实盘点出来的都是序列，所以说<strong>任何序列都是可迭代的对象，</strong> 其原因在于他们至少都会实现__ getittem __ 方法（序列都可以通过索引获取元素）。</p><h2 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h2><p>在介绍可迭代对象时候说到，当使用for循环时候，解释器会检查对象是否有__ iter __ ()方法，有的话就是调用它来获取一个迭代器。那么究竟什么是迭代器呢？</p><p>其实迭代器是实现了<strong>__ iter __ 方法和 __ next __ 方法的对象</strong>。__ iter __ 方法用于返回迭代器本身，而 __ next __ 用于返回下一个元素。我们自定义一个迭代器，以斐波那契数列为例说明一下其内部的执行情况，看示例5。</p><pre><code class="python"># 示例5import itertoolsclass Fib:    def __init__(self):        self.pre = 0        self.cur = 1    def __iter__(self):        return self    def __next__(self):        p = self.cur        self.cur += self.pre        self.pre = p        return pf = Fib()a = list(itertools.islice(f,0,10))print(a)#[1, 1, 2, 3, 5, 8, 13, 21, 34, 55]</code></pre><p>从示例5中__ iter __ ()返回了迭代器对象本身，以便在使用可迭代对象的地方(如for循环中)使用迭代器，而 __ next __ ()则通过计算返回下一个元素。我们再一起看看下面的示例6。</p><pre><code class="python">#示例6&gt;&gt;&gt;s = &#39;ABCD&#39;&gt;&gt;&gt;it = iter(s)&gt;&gt;&gt;while True:print(next(it))ABCD---------------------------------------------------------------------------StopIteration                             Traceback (most recent call last)&lt;ipython-input-4-d09d5cde4495&gt; in &lt;module&gt;()----&gt; 1 while True:print(next(it))StopIteration:&gt;&gt;&gt; list(it)[]&gt;&gt;&gt;list(iter(s))[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;]</code></pre><p>在示例6中先定义一个字符串的可迭代对象，通过iter()函数返回一个迭代器(会自动调用对象的__ iter __ 方法)，然后在循环中通过next()取值并打印(会自动调用对象 __ next __ ()方法)，通过next()方法一个个地遍历可迭代对象中的元素，当遍历结束，便会抛出<strong>StopIteratioin异常</strong>，这时迭代器也没用了，如果要再次迭代，就要使用iter()函数重新构建迭代器。</p><p>通过示例5和示例6，我们可知道<strong>迭代器是一个可以记住遍历位置的对象，其内部有一个状态用于记录迭代所在的位置，以便下次迭代时候能取出正确的元素</strong>。迭代器就像一个懒人一样，当你需要数据时候才会返回给你，否则就在等待下一次的调用。</p><p>如果要检查某个对象是否为迭代器，最好的方式是使用isinstance( )函数，见示例7。</p><pre><code class="python"># 示例7from collections.abc import Iterator f = Fib()print(isinstance(f,Iterator)) #True</code></pre><p>好了，说了那么多，究竟迭代器哪些用处呢？其实在Python语言内部，迭代器用于支持：</p><ul><li>for 循环</li><li>构建和扩展集合类型</li><li>逐行遍历文本文件</li><li>列表推导、字典推导和集合推导</li><li>元组拆包</li><li>调用函数时，使用*拆包</li></ul><h2 id="可迭代对象和迭代器的关系"><a href="#可迭代对象和迭代器的关系" class="headerlink" title="可迭代对象和迭代器的关系"></a>可迭代对象和迭代器的关系</h2><ul><li><p>可迭代对象不一定是迭代器，迭代器一定是可迭代对象。因为迭代器一定会实现 __ iter __ 方法，而可迭代对象尽管实现了 __ iter __ 也不一定实现 __ next __方法。</p></li><li><p>Python 从可迭代对象中获取迭代器，根据示例6的例子，我们知道先是使用iter()函数在迭代对象中获取迭代器，然后使用next()来获取下一个元素，关系如下图所示。</p><p><img src="https://gitee.com/weifagan/MyPic/raw/master/img/iter.png"></p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>可迭代对象实现了 __ iter __ 方法或者实现 __ getitem __方法而且其参数从0开始索引。</li><li>使用iter()函数判断可迭代对象更准确</li><li>任何序列都是可迭代对象</li><li>迭代器对象实现了 __ iter __ 和 __ next __方法。</li><li>迭代器是一个可以记住遍历位置的对象，其内部有一个状态用于记录迭代所在的位置，以便下次迭代时候能取出正确的元素</li><li>检查对象是否为迭代器最好的方式是调用isinstance()方法。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 可迭代对象、迭代器 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
